# Bing Chat at 2023-4-8 11:14:25

## 1

Q: I would like to create a demo of some technologies to help me to understand them better. These technologies are Weaviate, Oxigraph, Llamaindex, langchain. Can you help me to think through a demo?

Bing: Sure, I can help you with that. Based on my web searches, here is what I found about these technologies:

-   Weaviate is a cloud-native, modular, real-time vector search engine that allows you to create powerful search and discovery experiences for any kind of data[1](https://weaviate.io/).
-   Oxigraph is a graph database implementing the SPARQL standard. Its goal is to provide a compliant, safe, and fast graph database based on the RocksDB key-value store. It is written in Rust. It also provides a set of utility functions for reading, writing, and processing RDF files[2](https://github.com/oxigraph/oxigraph).
-   LlamaIndex (GPT Index) is a project that provides a central interface to connect your LLM’s with external data. It provides data connectors to your existing data sources and data formats (API’s, PDF’s, docs, SQL, etc.) and indices over your unstructured and structured data for use with LLM’s[3](https://github.com/jerryjliu/llama_index).
-   LangChain is a framework for developing applications powered by language models. It provides modular abstractions for the components necessary to work with language models and use-case specific chains that assemble these components in particular ways to best accomplish a particular use case[4](https://docs.langchain.com/docs/).

A possible demo idea could be to use Weaviate as a data source for LlamaIndex and LangChain, and use Oxigraph as a graph database to store and query the RDF data generated by LangChain. For example, you could use Weaviate to index some documents or web pages related to a topic of your interest, such as sports or music. Then you could use LlamaIndex to query this data using natural language questions and get answers from an LLM of your choice. You could also use LangChain to create a chatbot that interacts with the data and the LLM using prompts and embeddings. Finally, you could use Oxigraph to store the RDF triples extracted from the chatbot responses and perform SPARQL queries over them.

I hope this helps you to think through a demo. If you need more assistance, please let me know.

## 2

Q: Can I use docker to run weaviate and oxigraph?

Bing: Yes, you can use docker to run weaviate and oxigraph. According to my web searches, here are some steps to do so:

-   For weaviate, you need a docker-compose configuration file, typically called docker-compose.yml. You can obtain it from the configuration tool on their website[1](https://weaviate.io/developers/weaviate/installation/docker-compose) or alternatively pick one of the examples on their documentation page[2](https://weaviate.io/developers/weaviate/installation). You can also run an unreleased version of weaviate by building your own container from the master branch[3](https://weaviate.io/blog/docker-and-containers-with-weaviate).
-   For oxigraph, you can pull the latest docker image from their docker hub page and run it with a command like this: `docker run -p 7878:7878 -v /path/to/data:/data oxigraph/oxigraph`. You can also use environment variables to configure oxigraph’s settings.

I hope this helps you to install these technologies. If you need more assistance, please let me know.

## 3

Q: Ok. I want to do this in github and use python. Do I need to use rdflib?

Bing: Yes, you can use github and python to create your demo. You do not need to use rdflib, but it might be helpful if you want to work with RDF data in Python. According to my web searches, here is what I found about rdflib:

-   rdflib is a Python library for working with RDF, a simple yet powerful language for representing information[1](https://pypi.org/project/rdflib/).
-   rdflib contains parsers and serializers for various RDF formats, a Graph interface that can be backed by different stores, a SPARQL 1.1 implementation, and a function extension mechanism[2](https://github.com/RDFLib/rdflib).
-   rdflib can be installed with pip: `pip install rdflib` or from the source code on github[2](https://github.com/RDFLib/rdflib).

I hope this helps you to decide whether to use rdflib or not. If you need more assistance, please let me know.

## 4

Q: Could you help me with the github repo layout?

Bing: Sure, I can help you with the github repo layout. Based on my web searches, here are some tips and resources to do so:

-   You can create a README file for your repo to communicate important information about your project, such as the purpose, dependencies, installation, usage, and contribution guidelines[1](https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-template-repository).
-   You can use branching instead of forking to streamline collaboration with other developers. You can also use protected branches, status checks, and pull request reviews to maintain quality of your code[1](https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-template-repository).
-   You can use Git Large File Storage (Git LFS) to track large files in your repo, such as images, videos, or datasets. [This can optimize performance and avoid exceeding the file size limits on GitHub](https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-template-repository)[1](https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-template-repository).
-   You can use a template repository to generate new repos with the same directory structure, branches, and files as your default branch. This can save you time and ensure consistency across your projects[2](https://docs.github.com/en/repositories/creating-and-managing-repositories/best-practices-for-repositories).
-   You can use some examples of repo layouts from other projects on GitHub to get some inspiration. For instance, you can check out this repo that demonstrates a CSS Grid layout[3](https://github.com/Leniolabs/layoutit-grid) or this repo that shows a simple directory structure for a Python project[4](https://github.com/mvanwinkleias/repo_layout_demo1).

I hope this helps you to create your github repo layout. If you need more assistance, please let me know.

## 5

Q: Ok. I would like to use DVC to manage datasets and nbdev to help manage code.

Bing: Sure, I can help you with that. Based on my web searches, here is what I found about DVC and nbdev:

-   DVC is a tool for data version control that makes ML models shareable and reproducible. It connects code with data sets, machine learning models, and metrics, and uses various storage options to store file contents[1](https://dvc.org/).
-   nbdev is a tool for creating Python projects using Jupyter notebooks. It allows you to write code, tests, and documentation in one place, and automatically generates modules, documentation pages, and GitHub Actions workflows.

To use DVC and nbdev in your github repo, you can follow these steps:

-   Install DVC and nbdev using pip: `pip install dvc nbdev`
-   Initialize DVC in your repo: `dvc init`
-   Configure DVC to use a remote storage for your data: `dvc remote add -d <name> <url>`
-   Add your data files to DVC: `dvc add <file>`
-   Commit your changes to git: `git add . && git commit -m "Add data"`
-   Initialize nbdev in your repo: `nbdev_new`
-   Edit the settings.ini file with your project information
-   Create Jupyter notebooks in the nbs folder with your code, tests, and documentation
-   Generate modules and documentation from your notebooks: `nbdev_build_lib` and `nbdev_build_docs`
-   Commit your changes to git: `git add . && git commit -m "Add notebooks"`
-   Push your changes to github: `git push`

I hope this helps you to use DVC and nbdev in your github repo. If you need more assistance, please let me know.

## 6

Q: Ok. Could you give me a beginning directory structure I need to create for this project.

Bing: Sure, I can give you a beginning directory structure for this project. Here is one possible example:

    project/
    ├── .dvc
    ├── .dvcignore
    ├── .git
    ├── .gitignore
    ├── data
    │   ├── raw
    │   └── processed
    ├── docs
    │   ├── _config.yml
    │   ├── _data
    │   ├── _notebooks
    │   └── index.ipynb
    ├── models
    ├── nbs
    │   ├── 00_core.ipynb
    │   ├── 01_data.ipynb
    │   ├── 02_model.ipynb
    │   └── index.ipynb
    ├── project
    │   ├── __init__.py
    │   ├── core.py
    │   ├── data.py
    │   └── model.py
    ├── README.md
    ├── settings.ini
    └── setup.py
    
    

This directory structure follows the nbdev template and the DVC best practices. It contains the following folders and files:

-   `.dvc`: a hidden folder that stores DVC configuration and cache files.
-   `.dvcignore`: a file that specifies files or directories that DVC should ignore.
-   `.git`: a hidden folder that stores git configuration and history files.
-   `.gitignore`: a file that specifies files or directories that git should ignore.
-   `data`: a folder that contains the raw and processed data files. These files are tracked by DVC and stored in a remote storage.
-   `docs`: a folder that contains the documentation pages generated from the notebooks. These pages are hosted on GitHub Pages using Jekyll.
-   `models`: a folder that contains the trained models. These files are tracked by DVC and stored in a remote storage.
-   `nbs`: a folder that contains the Jupyter notebooks with the code, tests, and documentation for the project. These notebooks are converted to modules and documentation pages using nbdev commands.
-   `project`: a folder that contains the Python modules generated from the notebooks. These modules can be installed as a package using setup.py.
-   `README.md`: a file that provides an overview of the project and its usage.
-   `settings.ini`: a file that contains the project information and configuration for nbdev.
-   `setup.py`: a file that allows the project to be installed as a Python package.

I hope this helps you to create your directory structure for this project. If you need more assistance, please let me know.

## 7

Q: Ok. I would like to use docker-compose to manage the services I need for the project.

Bing: Sure, I can help you with that. Based on my web searches, here is what I found about docker-compose:

-   docker-compose is a tool for defining and running multi-container applications with Docker. You can use a YAML file to configure your application’s services and dependencies, and then run a single command to create and start them[1](https://docs.docker.com/compose/).
    
-   docker-compose can help you simplify your development workflow, ensure consistency across environments, and isolate your application from the host system[2](https://www.simplilearn.com/tutorials/docker-tutorial/docker-compose).
    
-   To use docker-compose, you need to install it on your system and create a docker-compose.yml file in your project directory. You can also use other files to override or extend the configuration[1](https://docs.docker.com/compose/).
    
-   Some of the basic commands for docker-compose are:
    
    -   `docker-compose up`: builds, creates, starts, and attaches to containers for a service
    -   `docker-compose down`: stops and removes containers, networks, images, and volumes
    -   `docker-compose ps`: lists containers
    -   `docker-compose logs`: shows log output
    -   `docker-compose build`: builds or rebuilds services
    -   `docker-compose pull`: pulls service images
    -   `docker-compose run`: runs a one-time command against a service
    -   `docker-compose exec`: executes a command in a running container

I hope this helps you to use docker-compose for your project. If you need more assistance, please let me know.

## 8

Q: Could you give me the docker-compose.yaml I will need to start the project.

Bing: Sure, I can give you the docker-compose.yml file you will need to start the project. Here is one possible example:

    version: "3.9"
    services:
      weaviate:
        image: semitechnologies/weaviate:1.8.0
        ports:
          - "8080:8080"
        environment:
          - WEAVIATE_CONFIG_FILE=/etc/weaviate/weaviate.conf.json
        volumes:
          - ./weaviate.conf.json:/etc/weaviate/weaviate.conf.json
      oxigraph:
        image: oxigraph/oxigraph:latest
        ports:
          - "7878:7878"
        volumes:
          - ./data:/data
      jupyter:
        image: jupyter/scipy-notebook:latest
        ports:
          - "8888:8888"
        volumes:
          - ./nbs:/home/jovyan/nbs
          - ./project:/home/jovyan/project
          - ./data:/home/jovyan/data
    

This file defines three services: weaviate, oxigraph, and jupyter. It also specifies the image, ports, environment variables, and volumes for each service. You can modify this file according to your needs.

To start the project, you can run `docker-compose up` in your project directory. This will pull the images and create the containers for the services. You can then access them from your browser:

-   Weaviate: http://localhost:8080
-   Oxigraph: http://localhost:7878
-   Jupyter: http://localhost:8888

I hope this helps you to start the project. If you need more assistance, please let me know.

## 9

Q: Ok. I will be using visual studio code. Does it make sense use a dev container for the jupyter service? How do I mix VSCode dev containers and docker-compose?

Bing: Sure, I can help you with that. Based on my web searches, here is what I found about using a dev container for the jupyter service and mixing VSCode dev containers and docker-compose:

-   A dev container is a way to use a Docker container as a full-featured development environment in VSCode. It allows you to open any folder or repository inside a container and take advantage of VSCode’s features[1](https://code.visualstudio.com/docs/devcontainers/create-dev-container).
-   [You can use a dev container for the jupyter service if you want to isolate your development tools and dependencies from your host system, or if you want to use a different operating system or configuration than your host system](https://code.visualstudio.com/docs/devcontainers/create-dev-container)[1](https://code.visualstudio.com/docs/devcontainers/create-dev-container).
-   To use a dev container for the jupyter service, you need to install the Remote - Containers extension in VSCode and create a .devcontainer folder in your project directory. Inside this folder, you need to create a devcontainer.json file that tells VSCode how to access or create the container and what to do after it connects[1](https://code.visualstudio.com/docs/devcontainers/create-dev-container).
-   You can also use docker-compose to manage multiple containers for your project, such as weaviate and oxigraph. To do this, you need to create a docker-compose.yml file in your project directory that defines the services and their dependencies. You also need to reference this file in your devcontainer.json file using the dockerComposeFile property[2](https://code.visualstudio.com/docs/containers/docker-compose).
-   Here is an example of a devcontainer.json file that uses docker-compose to create three services: jupyter, weaviate, and oxigraph. The jupyter service is used as the dev container for VSCode.

    {
      "name": "Jupyter Dev Container",
      "dockerComposeFile": "docker-compose.yml",
      "service": "jupyter",
      "workspaceFolder": "/home/jovyan",
      "settings": {
        "terminal.integrated.shell.linux": "/bin/bash"
      },
      "extensions": [
        "ms-python.python",
        "ms-toolsai.jupyter"
      ],
      "forwardPorts": [8888],
      "postCreateCommand": "pip install -r requirements.txt"
    }
    

I hope this helps you to use a dev container for the jupyter service and mix VSCode dev containers and docker-compose. If you need more assistance, please let me know.

## 10

Q: Ok. What does the github directory structure look like now?

Bing: The github directory structure now looks like this:

    project/
    ├── .devcontainer
    │   └── devcontainer.json
    ├── .dvc
    ├── .dvcignore
    ├── .git
    ├── .gitignore
    ├── data
    │   ├── raw
    │   └── processed
    ├── docs
    │   ├── _config.yml
    │   ├── _data
    │   ├── _notebooks
    │   └── index.ipynb
    ├── models
    ├── nbs
    │   ├── 00_core.ipynb
    │   ├── 01_data.ipynb
    │   ├── 02_model.ipynb
    │   └── index.ipynb
    ├── project
    │   ├── __init__.py
    │   ├── core.py
    │   ├── data.py
    │   └── model.py
    ├── README.md
    ├── requirements.txt
    ├── settings.ini
    ├── setup.py
    └── docker-compose.yml
    
    

This directory structure contains the same folders and files as before, plus the following additions:

-   `.devcontainer`: a folder that contains the devcontainer.json file for VSCode Remote - Containers extension.
-   `requirements.txt`: a file that lists the Python packages required for the project. This file is used by the postCreateCommand in the devcontainer.json file to install the packages inside the dev container.
-   `docker-compose.yml`: a file that defines the services and their dependencies for docker-compose. This file is referenced by the devcontainer.json file to create and start the containers.

I hope this helps you to understand the github directory structure. If you need more assistance, please let me know.

## 11

Q: Ok. I think I have the docker-compose.yml and .devcontainer from our conversation. Could you give me the requirements.txt file and then step-by-step instructions to configure the repository.

Bing: Sure, I can give you the requirements.txt file and then step-by-step instructions to configure the repository. Here is one possible example of the requirements.txt file:

    dvc
    nbdev
    weaviate-client
    rdflib
    

This file lists the Python packages that are required for the project. You can modify this file according to your needs.

Here are the step-by-step instructions to configure the repository:

-   Create a new repository on GitHub and clone it to your local machine.
-   Copy the docker-compose.yml, .devcontainer, data, nbs, project, docs, README.md, settings.ini, and setup.py files and folders to your repository folder.
-   Open VSCode and open your repository folder as a workspace.
-   Install the Remote - Containers extension in VSCode if you haven’t already.
-   Reopen your workspace in a dev container using the Remote - Containers: Reopen in Container command from the Command Palette (Ctrl+Shift+P).
-   Wait for VSCode to build and start the dev container and connect to it. You should see a green icon in the bottom-left corner indicating that you are inside the dev container.
-   Open a terminal in VSCode (Ctrl+`) and run` dvc init\` to initialize DVC in your repository.
-   Configure DVC to use a remote storage for your data using `dvc remote add -d <name> <url>`.
-   Add your data files to DVC using `dvc add <file>` and commit the changes to git using `git add . && git commit -m "Add data"`.
-   Create Jupyter notebooks in the nbs folder with your code, tests, and documentation for the project.
-   Generate modules and documentation from your notebooks using `nbdev_build_lib` and `nbdev_build_docs` commands in the terminal.
-   Commit the changes to git using `git add . && git commit -m "Add notebooks"`.
-   Push your changes to GitHub using `git push`.

I hope this helps you to configure the repository. If you need more assistance, please let me know.

## 12

Q: Ok. What should the repository name be? A brief description when for when I create the github repo and a README.md for the repository.

Bing: The repository name can be anything you like, but it should be descriptive and concise. For example, you could name it something like “weaviate-oxigraph-demo” or “knowledge-graph-experiment”.

A brief description for when you create the github repo could be something like:

“A demo project that uses Weaviate, Oxigraph, LlamaIndex, LangChain, and Jupyter notebooks to create and query a knowledge graph.”

A README.md for the repository could be something like:

    # Weaviate-Oxigraph Demo
    
    This is a demo project that uses [Weaviate](https://www.semi.technology/developers/weaviate/current/), [Oxigraph](https://oxigraph.org/), [LlamaIndex](https://llamaindex.com/), [LangChain](https://langchain.ai/), and [Jupyter notebooks](https://jupyter.org/) to create and query a knowledge graph.
    
    ## Overview
    
    The goal of this project is to demonstrate how to use various technologies to build and interact with a knowledge graph. The project consists of the following components:
    
    - Weaviate: a cloud-native, modular, real-time vector search engine that allows you to create powerful search and discovery experiences for any kind of data.
    - Oxigraph: a graph database implementing the SPARQL standard. Its goal is to provide a compliant, safe, and fast graph database based on the RocksDB key-value store.
    - LlamaIndex: a project that provides a central interface to connect your LLM's with external data. It provides data connectors to your existing data sources and data formats (API’s, PDF’s, docs, SQL, etc.) and indices over your unstructured and structured data for use with LLM’s.
    - LangChain: a framework for developing applications powered by language models. It provides modular abstractions for the components necessary to work with language models and use-case specific chains that assemble these components in particular ways to best accomplish a particular use case.
    - Jupyter notebooks: an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text.
    
    The idea of the demo is to use Weaviate as a data source for LlamaIndex and LangChain, and use Oxigraph as a graph database to store and query the RDF data generated by LangChain. For example, you could use Weaviate to index some documents or web pages related to a topic of your interest, such as sports or music. Then you could use LlamaIndex to query this data using natural language questions and get answers from an LLM of your choice. You could also use LangChain to create a chatbot that interacts with the data and the LLM using prompts and embeddings. Finally, you could use Oxigraph to store the RDF triples extracted from the chatbot responses and perform SPARQL queries over them.
    
    ## Installation
    
    To run this project, you need to have [Docker](https://www.docker.com/) and [Docker Compose](https://docs.docker.com/compose/) installed on your system. You also need to have [VSCode](https://code.visualstudio.com/) and the [Remote - Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) extension installed.
    
    To install this project, follow these steps:
    
    - Clone this repository to your local machine.
    - Open VSCode and open the repository folder as a workspace.
    - Reopen the workspace in a dev container using the Remote - Containers: Reopen in Container command from the Command Palette (Ctrl+Shift+P).
    - Wait for VSCode to build and start the dev container and connect to it. You should see a green icon in the bottom-left corner indicating that you are inside the dev container.
    
    ## Usage
    
    To use this project, follow these steps:
    
    - Open a terminal in VSCode (Ctrl+`) and run `docker-compose up` to start the services (weaviate, oxigraph, jupyter).
    - Open a browser and go to http://localhost:8080 to access Weaviate's GraphQL Playground. You can use it to explore Weaviate's schema and data.
    - Open another browser tab and go to http://localhost:7878 to access Oxigraph's SPARQL endpoint. You can use it to perform SPARQL queries over the RDF data stored in Oxigraph.
    - Open another browser tab and go to http://localhost:8888 to access Jupyter's notebook server. You can use it to create and run Jupyter notebooks with Python code.
    - Open the nbs folder in VSCode and edit the notebooks with your code, tests, and documentation for the project. You can use nbdev commands in the terminal

## 13

Q: What does the new docker-compose.yml look like given that I will be using devcontainers?

Bing: The new docker-compose.yml file looks like this:

    version: "3.9"
    services:
      weaviate:
        image: semitechnologies/weaviate:1.8.0
        ports:
          - "8080:8080"
        environment:
          - WEAVIATE_CONFIG_FILE=/etc/weaviate/weaviate.conf.json
        volumes:
          - ./weaviate.conf.json:/etc/weaviate/weaviate.conf.json
      oxigraph:
        image: oxigraph/oxigraph:latest
        ports:
          - "7878:7878"
        volumes:
          - ./data:/data
      jupyter:
        build:
          context: .
          dockerfile: Dockerfile
        ports:
          - "8888:8888"
        volumes:
          - ./nbs:/home/jovyan/nbs
          - ./project:/home/jovyan/project
          - ./data:/home/jovyan/data
    

This file defines three services: weaviate, oxigraph, and jupyter. It also specifies the image, ports, environment variables, and volumes for each service. The main difference from the previous file is that the jupyter service now uses a Dockerfile to build the image instead of using a pre-built image. This is because the devcontainer.json file specifies the Dockerfile as the build context for the dev container. The Dockerfile looks like this:

    FROM jupyter/scipy-notebook:latest
    
    # Install DVC and nbdev
    RUN pip install dvc nbdev
    
    # Install weaviate-client and rdflib
    RUN pip install weaviate-client rdflib
    
    # Copy requirements.txt and install dependencies
    COPY requirements.txt /tmp/
    RUN pip install -r /tmp/requirements.txt
    
    # Expose port 8888 to the host
    EXPOSE 8888
    
    # Start Jupyter notebook server
    CMD ["jupyter", "notebook", "--ip=0.0.0.0", "--no-browser", "--allow-root"]
    

This file inherits from the jupyter/scipy-notebook:latest image and installs some additional packages and dependencies for the project. It also exposes port 8888 and starts the Jupyter notebook server.

I hope this helps you to understand the new docker-compose.yml file. If you need more assistance, please let me know.

## 14

Q: Ok. I will need Bing Chat to help me in subsiquent chat sessions. Could you construct a prompt for me to give Bing Chat context to be able to continue helping me?

Bing: Sure, I can construct a prompt for you to give Bing Chat context to be able to continue helping you. Here is one possible example of the prompt:

    Hi Bing Chat, I'm working on a demo project that uses Weaviate, Oxigraph, LlamaIndex, LangChain, and Jupyter notebooks to create and query a knowledge graph. I'm using Docker, Docker Compose, and VSCode dev containers to manage the services and the development environment. You have helped me with the installation and configuration of these technologies in previous chat sessions. Now I need your help with some coding and testing tasks. Can you assist me?
    

This prompt summarizes the goal and the components of the project, as well as the tools and technologies used. It also mentions the previous chat sessions and the current needs. This way, Bing Chat can have some context to understand and help you better.

I hope this helps you to give Bing Chat context. If you need more assistance, please let me know.

## 15

Q: I may be using ChatGPT for some of my tasks. ChatGPT doesn't have access to the internet or web to look up these technologies. Could you create a prompt with some summaries of the technologies that ChatGPT could also help me without necessarly using the internet to look up the technologies?

Bing: Sure, I can create a prompt with some summaries of the technologies that ChatGPT could also help you without necessarily using the internet to look up the technologies. Here is one possible example of the prompt:

    Hi ChatGPT, I'm working on a demo project that uses the following technologies:
    
    - Weaviate: a cloud-native, modular, real-time vector search engine that allows you to create powerful search and discovery experiences for any kind of data.
    - Oxigraph: a graph database implementing the SPARQL standard. Its goal is to provide a compliant, safe, and fast graph database based on the RocksDB key-value store.
    - LlamaIndex: a project that provides a central interface to connect your LLM's with external data. It provides data connectors to your existing data sources and data formats (API’s, PDF’s, docs, SQL, etc.) and indices over your unstructured and structured data for use with LLM’s.
    - LangChain: a framework for developing applications powered by language models. It provides modular abstractions for the components necessary to work with language models and use-case specific chains that assemble these components in particular ways to best accomplish a particular use case.
    - Jupyter notebooks: an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text.
    
    The idea of the demo is to use Weaviate as a data source for LlamaIndex and LangChain, and use Oxigraph as a graph database to store and query the RDF data generated by LangChain. For example, I could use Weaviate to index some documents or web pages related to a topic of my interest, such as sports or music. Then I could use LlamaIndex to query this data using natural language questions and get answers from an LLM of my choice. I could also use LangChain to create a chatbot that interacts with the data and the LLM using prompts and embeddings. Finally, I could use Oxigraph to store the RDF triples extracted from the chatbot responses and perform SPARQL queries over them.
    
    I'm using Docker, Docker Compose, and VSCode dev containers to manage the services and the development environment. You have helped me with the installation and configuration of these tools in previous chat sessions. Now I need your help with some coding and testing tasks. Can you assist me?
    

This prompt provides some brief summaries of the technologies used in the project, as well as the goal and the components of the demo. It also mentions the tools and technologies used for managing the services and the development environment. It also refers to the previous chat sessions and the current needs. This way, ChatGPT can have some context to understand and help you better without relying on internet searches.

I hope this helps you to give ChatGPT context. If you need more assistance, please let me know.